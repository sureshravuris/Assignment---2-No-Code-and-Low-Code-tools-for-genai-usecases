{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMnVFqWhDVPXfpI658DZGjx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "597a71128f0245ee9bf09bbb2a4ce43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3018d8c138314e768c4b873b10564fea",
              "IPY_MODEL_c7f2eb99c8494d52a7539526b873ce69",
              "IPY_MODEL_2726e13ad97d41ac868cc3b0924a239b"
            ],
            "layout": "IPY_MODEL_e66257d109464a72bb7bcc948747894b"
          }
        },
        "3018d8c138314e768c4b873b10564fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8551f98d1ea74017ba7d0d1fd8f80615",
            "placeholder": "​",
            "style": "IPY_MODEL_aca1b1bb995b43bfb7240aba3f99ff02",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7f2eb99c8494d52a7539526b873ce69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_396a46f37c3549a5b7ce84c6263b0874",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a37a1988382480ba7b91b4b29fcdd14",
            "value": 2
          }
        },
        "2726e13ad97d41ac868cc3b0924a239b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c61ab84c59474a3fad6392e74ff95691",
            "placeholder": "​",
            "style": "IPY_MODEL_34c040748e0e4f6da5bf32e8288162e0",
            "value": " 2/2 [00:02&lt;00:00,  1.07s/it]"
          }
        },
        "e66257d109464a72bb7bcc948747894b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8551f98d1ea74017ba7d0d1fd8f80615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca1b1bb995b43bfb7240aba3f99ff02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "396a46f37c3549a5b7ce84c6263b0874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a37a1988382480ba7b91b4b29fcdd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c61ab84c59474a3fad6392e74ff95691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c040748e0e4f6da5bf32e8288162e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schumbar/CMPE297/blob/main/assignment_02/part_a/ShawnChumbar_Assignment02_PartA_Fintetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 02 Part A - Llama Factory Finetuning\n",
        "\n",
        "## Assignment Description\n",
        "Part A: Demonstrate (ideally with qlora or lora)\n",
        "1. demonstrate supervised fine tuning (lora/qlora any is fine)\n",
        "2. dpo training\n",
        "3. ppo training\n",
        "\n",
        "## References\n",
        "\n",
        "Please see below for the references that were used for this assignment.\n",
        "\n",
        "1. [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)\n",
        "2. [LLaMA-Factory Colab Notebook](https://colab.research.google.com/drive/1fvw1MR3o-03qQ9eRw09glkN2VqIybKNm?usp=sharing)\n",
        "3. [LLaMA-Factory PyPI](https://pypi.org/project/llamafactory/)"
      ],
      "metadata": {
        "id": "E1A25uUCxOl-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwMiP7jDdAL1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVvJYwne3hha"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pip Install Dependencies"
      ],
      "metadata": {
        "id": "5HvFzgsq9iFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXsaUJuS9hKc",
        "outputId": "d62ecb6e-cf0c-4f5b-98cb-9feb37ca72db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.4.0+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.33.0)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.8.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: xxhash, shtab, pyarrow, dill, multiprocess, tyro, datasets, trl\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 shtab-1.7.1 trl-0.10.1 tyro-0.8.10 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yUF4Hk5dOoz"
      },
      "source": [
        "### Install LLaMA Factory\n",
        "\n",
        "Install LLaMA Factory from source on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4pY14h6_bDrr",
        "outputId": "55ee254a-adc7-46eb-8791-5a6800a0c750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 17234, done.\u001b[K\n",
            "remote: Counting objects: 100% (978/978), done.\u001b[K\n",
            "remote: Compressing objects: 100% (473/473), done.\u001b[K\n",
            "remote: Total 17234 (delta 681), reused 730 (delta 505), pack-reused 16256 (from 1)\u001b[K\n",
            "Receiving objects: 100% (17234/17234), 225.95 MiB | 25.27 MiB/s, done.\n",
            "Resolving deltas: 100% (12577/12577), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.45.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.2)\n",
            "Collecting datasets<=2.21.0,>=2.16.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.33.0)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.9.1.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.9.1.dev0)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.9.1)\n",
            "Collecting fastapi (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fastapi-0.114.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.1.dev0)\n",
            "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.4.0+cu121)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.0.7)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->llamafactory==0.9.1.dev0)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.8.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.1.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (13.8.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading fastapi-0.114.1-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22354 sha256=85ac32cdb404fb7a04734e86219561d68f44c41896881b9492247c07bc62a8e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-czzvjisg/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=dad6c226a493c46d264820fb23b32cd07762187d10a65ae85feeb2cda0fbd43f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, fire, ffmpy, aiofiles, uvicorn, tiktoken, starlette, httpcore, sse-starlette, httpx, fastapi, bitsandbytes, gradio-client, datasets, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.0.0\n",
            "    Uninstalling datasets-3.0.0:\n",
            "      Successfully uninstalled datasets-3.0.0\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.10.1\n",
            "    Uninstalling trl-0.10.1:\n",
            "      Successfully uninstalled trl-0.10.1\n",
            "Successfully installed aiofiles-23.2.1 bitsandbytes-0.43.3 datasets-2.21.0 fastapi-0.114.1 ffmpy-0.4.0 fire-0.6.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 llamafactory-0.9.1.dev0 orjson-3.10.7 peft-0.12.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.4 semantic-version-2.10.0 sse-starlette-2.1.3 starlette-0.38.5 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9D2DY5dqmw"
      },
      "source": [
        "## Finetune Gemma - LORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gWIzVxhwcDSw",
        "outputId": "22606640-3c42-4f40-f23a-8c89ae4f0014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-09-12 03:47:28.197927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-12 03:47:28.217951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-12 03:47:28.224033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-12 03:47:28.238335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-12 03:47:29.363235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "09/12/2024 03:47:35 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "09/12/2024 03:47:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "config.json: 100% 627/627 [00:00<00:00, 4.76MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:47:35,817 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:47:35,820 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 33.6k/33.6k [00:00<00:00, 10.4MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 42.1MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 223MB/s]\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 6.16MB/s]\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:37,133 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:37,133 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:37,133 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:37,133 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:37,133 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:47:38,477 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:47:38,478 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:38,636 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:38,636 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:38,636 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:38,636 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:47:38,636 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "09/12/2024 03:47:39 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...\n",
            "Generating train split: 1000 examples [00:00, 39539.06 examples/s]\n",
            "Converting format of dataset: 100% 500/500 [00:00<00:00, 14406.68 examples/s]\n",
            "Running tokenizer on dataset: 100% 500/500 [00:00<00:00, 1433.01 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 50721, 476, 2185, 576, 3547, 223482, 235265, 107, 108, 106, 2516, 108, 31119, 223482, 603, 671, 3980, 578, 15855, 2185, 235341, 5698, 708, 4065, 235290, 1547, 235290, 8847, 12027, 611, 1368, 577, 1501, 1174, 235292, 109, 235274, 235265, 96352, 861, 15555, 235265, 1699, 6859, 223482, 235269, 692, 235303, 529, 1476, 235292, 235248, 235274, 7190, 832, 235290, 38197, 16251, 235269, 235248, 235284, 12776, 235269, 235248, 235274, 235283, 235284, 7190, 9512, 235269, 235248, 235274, 235283, 235284, 7190, 2003, 235269, 235248, 235274, 235283, 235310, 30882, 9193, 235269, 578, 235248, 235284, 43212, 47362, 10605, 235265, 109, 235284, 235265, 19392, 573, 7450, 235292, 878, 476, 2910, 25373, 14581, 235269, 80196, 3584, 573, 16251, 578, 573, 12776, 235265, 142473, 1843, 573, 9512, 578, 2003, 235269, 52174, 16263, 577, 7433, 674, 1104, 708, 793, 127675, 235265, 4463, 9193, 578, 47362, 10605, 235269, 578, 7345, 1578, 235265, 109, 235304, 235265, 4371, 573, 7450, 2066, 235292, 1927, 692, 798, 235269, 2142, 573, 7450, 5045, 604, 671, 6370, 689, 712, 235265, 1417, 877, 1707, 573, 16251, 577, 33398, 573, 10177, 578, 1501, 573, 223482, 978, 17580, 235265, 109, 235310, 235265, 21046, 861, 2959, 235292, 127940, 476, 2173, 235290, 33107, 2959, 1163, 8890, 6719, 235265, 226437, 10605, 573, 2959, 689, 1281, 15983, 13770, 577, 7704, 573, 223482, 774, 49004, 235265, 109, 235308, 235265, 12456, 573, 7450, 235292, 12266, 476, 221363, 689, 476, 21385, 7190, 235269, 1982, 476, 2301, 3619, 576, 7450, 591, 11082, 235248, 235274, 235283, 235310, 7190, 235275, 10401, 573, 5086, 576, 573, 2959, 235265, 68122, 52777, 573, 2959, 575, 476, 20856, 8252, 577, 8151, 573, 7450, 48145, 578, 108585, 1163, 573, 6837, 576, 573, 2959, 235265, 109, 235318, 235265, 7110, 573, 112137, 235292, 7110, 573, 112137, 604, 235248, 235274, 235290, 235284, 4363, 3274, 573, 6837, 603, 34620, 13658, 235265, 127401, 98285, 573, 17370, 675, 476, 128765, 578, 24740, 573, 112137, 1163, 577, 3425, 573, 1156, 2857, 604, 2550, 9428, 235265, 109, 235324, 235265, 17691, 578, 17399, 235292, 138056, 9499, 573, 112137, 10401, 476, 8811, 235269, 578, 1492, 17399, 573, 2185, 675, 573, 11548, 7450, 235265, 24250, 577, 582, 235290, 54541, 573, 2959, 1865, 1853, 112137, 1013, 4647, 235265, 109, 235321, 235265, 31831, 578, 8106, 235292, 31831, 861, 26405, 223482, 675, 861, 13142, 20585, 235269, 1582, 685, 6422, 9471, 235269, 72504, 9450, 235269, 202770, 235269, 689, 12578, 578, 13624, 235265, 22812, 689, 13528, 235269, 578, 8106, 7544, 235265, 23646, 235341, 1]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Describe a process of making crepes.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<eos>\n",
            "label_ids:\n",
            "[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 31119, 223482, 603, 671, 3980, 578, 15855, 2185, 235341, 5698, 708, 4065, 235290, 1547, 235290, 8847, 12027, 611, 1368, 577, 1501, 1174, 235292, 109, 235274, 235265, 96352, 861, 15555, 235265, 1699, 6859, 223482, 235269, 692, 235303, 529, 1476, 235292, 235248, 235274, 7190, 832, 235290, 38197, 16251, 235269, 235248, 235284, 12776, 235269, 235248, 235274, 235283, 235284, 7190, 9512, 235269, 235248, 235274, 235283, 235284, 7190, 2003, 235269, 235248, 235274, 235283, 235310, 30882, 9193, 235269, 578, 235248, 235284, 43212, 47362, 10605, 235265, 109, 235284, 235265, 19392, 573, 7450, 235292, 878, 476, 2910, 25373, 14581, 235269, 80196, 3584, 573, 16251, 578, 573, 12776, 235265, 142473, 1843, 573, 9512, 578, 2003, 235269, 52174, 16263, 577, 7433, 674, 1104, 708, 793, 127675, 235265, 4463, 9193, 578, 47362, 10605, 235269, 578, 7345, 1578, 235265, 109, 235304, 235265, 4371, 573, 7450, 2066, 235292, 1927, 692, 798, 235269, 2142, 573, 7450, 5045, 604, 671, 6370, 689, 712, 235265, 1417, 877, 1707, 573, 16251, 577, 33398, 573, 10177, 578, 1501, 573, 223482, 978, 17580, 235265, 109, 235310, 235265, 21046, 861, 2959, 235292, 127940, 476, 2173, 235290, 33107, 2959, 1163, 8890, 6719, 235265, 226437, 10605, 573, 2959, 689, 1281, 15983, 13770, 577, 7704, 573, 223482, 774, 49004, 235265, 109, 235308, 235265, 12456, 573, 7450, 235292, 12266, 476, 221363, 689, 476, 21385, 7190, 235269, 1982, 476, 2301, 3619, 576, 7450, 591, 11082, 235248, 235274, 235283, 235310, 7190, 235275, 10401, 573, 5086, 576, 573, 2959, 235265, 68122, 52777, 573, 2959, 575, 476, 20856, 8252, 577, 8151, 573, 7450, 48145, 578, 108585, 1163, 573, 6837, 576, 573, 2959, 235265, 109, 235318, 235265, 7110, 573, 112137, 235292, 7110, 573, 112137, 604, 235248, 235274, 235290, 235284, 4363, 3274, 573, 6837, 603, 34620, 13658, 235265, 127401, 98285, 573, 17370, 675, 476, 128765, 578, 24740, 573, 112137, 1163, 577, 3425, 573, 1156, 2857, 604, 2550, 9428, 235265, 109, 235324, 235265, 17691, 578, 17399, 235292, 138056, 9499, 573, 112137, 10401, 476, 8811, 235269, 578, 1492, 17399, 573, 2185, 675, 573, 11548, 7450, 235265, 24250, 577, 582, 235290, 54541, 573, 2959, 1865, 1853, 112137, 1013, 4647, 235265, 109, 235321, 235265, 31831, 578, 8106, 235292, 31831, 861, 26405, 223482, 675, 861, 13142, 20585, 235269, 1582, 685, 6422, 9471, 235269, 72504, 9450, 235269, 202770, 235269, 689, 12578, 578, 13624, 235265, 22812, 689, 13528, 235269, 578, 8106, 7544, 235265, 23646, 235341, 1]\n",
            "labels:\n",
            "<eos>Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<eos>\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:47:40,635 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:47:40,636 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "09/12/2024 03:47:40 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "model.safetensors.index.json: 100% 13.5k/13.5k [00:00<00:00, 41.1MB/s]\n",
            "[INFO|modeling_utils.py:3678] 2024-09-12 03:47:41,549 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.95G [00:00<00:12, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.95G [00:00<00:12, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.95G [00:00<00:11, 405MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.95G [00:00<00:11, 406MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.95G [00:00<00:11, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.95G [00:00<00:11, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.95G [00:00<00:11, 403MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.95G [00:00<00:11, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.95G [00:00<00:11, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.95G [00:01<00:11, 404MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.95G [00:01<00:10, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.95G [00:01<00:10, 407MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.95G [00:01<00:10, 406MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.95G [00:01<00:10, 404MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.95G [00:01<00:10, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 671M/4.95G [00:01<00:10, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.95G [00:01<00:10, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.95G [00:01<00:10, 399MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.95G [00:01<00:10, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.95G [00:02<00:10, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.95G [00:02<00:10, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.95G [00:02<00:10, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 965M/4.95G [00:02<00:10, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.95G [00:02<00:10, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.95G [00:02<00:10, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.95G [00:02<00:10, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.95G [00:02<00:09, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.95G [00:02<00:09, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.22G/4.95G [00:03<00:09, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.95G [00:03<00:09, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.95G [00:03<00:09, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.95G [00:03<00:09, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.95G [00:03<00:09, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.95G [00:03<00:08, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.95G [00:03<00:08, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.51G/4.95G [00:03<00:08, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.95G [00:03<00:08, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.95G [00:04<00:08, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.95G [00:04<00:08, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.95G [00:04<00:08, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.95G [00:04<00:08, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.76G/4.95G [00:04<00:08, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.95G [00:04<00:08, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.95G [00:04<00:08, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.95G [00:04<00:08, 378MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.95G [00:04<00:07, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.95G [00:05<00:07, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.01G/4.95G [00:05<00:07, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.06G/4.95G [00:05<00:07, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.95G [00:05<00:07, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.95G [00:05<00:07, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.95G [00:05<00:07, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.95G [00:05<00:07, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.95G [00:05<00:07, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.31G/4.95G [00:05<00:06, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.95G [00:06<00:06, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.95G [00:06<00:06, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.95G [00:06<00:06, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.95G [00:06<00:06, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.95G [00:06<00:06, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.56G/4.95G [00:06<00:06, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.60G/4.95G [00:06<00:05, 393MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.95G [00:06<00:05, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.95G [00:06<00:05, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.95G [00:07<00:05, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.95G [00:07<00:05, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.95G [00:07<00:05, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.85G/4.95G [00:07<00:05, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.89G/4.95G [00:07<00:05, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.95G [00:07<00:05, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.95G [00:07<00:05, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.95G [00:07<00:04, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.95G [00:07<00:04, 393MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.10G/4.95G [00:07<00:04, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.15G/4.95G [00:08<00:04, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.95G [00:08<00:04, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.95G [00:08<00:04, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.95G [00:08<00:04, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.95G [00:08<00:04, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.36G/4.95G [00:08<00:04, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.40G/4.95G [00:08<00:03, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.44G/4.95G [00:08<00:03, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.95G [00:08<00:03, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.95G [00:09<00:03, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.95G [00:09<00:03, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.95G [00:09<00:03, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.65G/4.95G [00:09<00:03, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.69G/4.95G [00:09<00:03, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.95G [00:09<00:03, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.95G [00:09<00:02, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.95G [00:09<00:02, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.95G [00:09<00:02, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.90G/4.95G [00:10<00:02, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.94G/4.95G [00:10<00:02, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.98G/4.95G [00:10<00:02, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.95G [00:10<00:02, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.95G [00:10<00:02, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.95G [00:10<00:02, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.95G [00:10<00:02, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.19G/4.95G [00:10<00:01, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.24G/4.95G [00:10<00:01, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.28G/4.95G [00:11<00:01, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.95G [00:11<00:01, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.95G [00:11<00:01, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.95G [00:11<00:01, 400MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.45G/4.95G [00:11<00:01, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.49G/4.95G [00:11<00:01, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.53G/4.95G [00:11<00:01, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.95G [00:11<00:00, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.95G [00:11<00:00, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.95G [00:11<00:00, 401MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.70G/4.95G [00:12<00:00, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.74G/4.95G [00:12<00:00, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.78G/4.95G [00:12<00:00, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.82G/4.95G [00:12<00:00, 407MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.95G [00:12<00:00, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.95G [00:12<00:00, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.95G [00:12<00:00, 385MB/s]\n",
            "Downloading shards:  50% 1/2 [00:12<00:12, 12.97s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/67.1M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 67.1M/67.1M [00:00<00:00, 409MB/s]\n",
            "Downloading shards: 100% 2/2 [00:13<00:00,  6.69s/it]\n",
            "[INFO|modeling_utils.py:1606] 2024-09-12 03:47:54,932 >> Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:47:54,933 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-09-12 03:47:55,357 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.87s/it]\n",
            "[INFO|modeling_utils.py:4507] 2024-09-12 03:47:59,577 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-09-12 03:47:59,577 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 1.24MB/s]\n",
            "[INFO|configuration_utils.py:993] 2024-09-12 03:47:59,804 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:47:59,804 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "09/12/2024 03:47:59 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "09/12/2024 03:47:59 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "09/12/2024 03:47:59 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "09/12/2024 03:47:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "09/12/2024 03:47:59 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,up_proj,o_proj,down_proj,v_proj,gate_proj\n",
            "09/12/2024 03:48:00 - INFO - llamafactory.model.loader - trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "[INFO|trainer.py:648] 2024-09-12 03:48:00,337 >> Using auto half precision backend\n",
            "09/12/2024 03:48:00 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2134] 2024-09-12 03:48:00,892 >> ***** Running training *****\n",
            "[INFO|trainer.py:2135] 2024-09-12 03:48:00,892 >>   Num examples = 500\n",
            "[INFO|trainer.py:2136] 2024-09-12 03:48:00,892 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2137] 2024-09-12 03:48:00,892 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2140] 2024-09-12 03:48:00,892 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2141] 2024-09-12 03:48:00,892 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2142] 2024-09-12 03:48:00,892 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2143] 2024-09-12 03:48:00,895 >>   Number of trainable parameters = 9,805,824\n",
            "  0% 0/186 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "{'loss': 1.3759, 'grad_norm': 0.5061381459236145, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.16}\n",
            "{'loss': 1.3239, 'grad_norm': 0.7071205377578735, 'learning_rate': 4.999557652060729e-05, 'epoch': 0.32}\n",
            "{'loss': 1.2056, 'grad_norm': 1.0549277067184448, 'learning_rate': 4.946665048328287e-05, 'epoch': 0.48}\n",
            "{'loss': 1.2348, 'grad_norm': 1.111830472946167, 'learning_rate': 4.807442755497524e-05, 'epoch': 0.64}\n",
            "{'loss': 1.1899, 'grad_norm': 0.9544026851654053, 'learning_rate': 4.586803181690609e-05, 'epoch': 0.8}\n",
            "{'loss': 1.2299, 'grad_norm': 0.6216996908187866, 'learning_rate': 4.292531514268008e-05, 'epoch': 0.96}\n",
            "{'loss': 1.0281, 'grad_norm': 0.5975288152694702, 'learning_rate': 3.9350110223152844e-05, 'epoch': 1.12}\n",
            "{'loss': 0.9498, 'grad_norm': 0.6828083992004395, 'learning_rate': 3.526856686758269e-05, 'epoch': 1.28}\n",
            "{'loss': 0.9058, 'grad_norm': 0.6734941601753235, 'learning_rate': 3.082470085335133e-05, 'epoch': 1.44}\n",
            "{'loss': 0.9432, 'grad_norm': 0.7914012670516968, 'learning_rate': 2.6175312381477442e-05, 'epoch': 1.6}\n",
            "{'loss': 0.9104, 'grad_norm': 0.6159288883209229, 'learning_rate': 2.148445343837755e-05, 'epoch': 1.76}\n",
            "{'loss': 1.0051, 'grad_norm': 0.7691546678543091, 'learning_rate': 1.69176392810087e-05, 'epoch': 1.92}\n",
            "{'loss': 0.9169, 'grad_norm': 0.6153213381767273, 'learning_rate': 1.2636008291040618e-05, 'epoch': 2.08}\n",
            "{'loss': 0.7614, 'grad_norm': 1.2935779094696045, 'learning_rate': 8.790636265485334e-06, 'epoch': 2.24}\n",
            "{'loss': 0.7117, 'grad_norm': 0.9486101269721985, 'learning_rate': 5.51720576197794e-06, 'epoch': 2.4}\n",
            "{'loss': 0.772, 'grad_norm': 1.1389315128326416, 'learning_rate': 2.931218588927315e-06, 'epoch': 2.56}\n",
            "{'loss': 0.7153, 'grad_norm': 1.7748827934265137, 'learning_rate': 1.1239203660860648e-06, 'epoch': 2.72}\n",
            "{'loss': 0.6757, 'grad_norm': 0.6542404294013977, 'learning_rate': 1.5908095594207583e-07, 'epoch': 2.88}\n",
            "100% 186/186 [08:23<00:00,  2.58s/it][INFO|trainer.py:3503] 2024-09-12 03:56:24,593 >> Saving model checkpoint to gemma_lora/checkpoint-186\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:24,871 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:24,872 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-09-12 03:56:24,976 >> tokenizer config file saved in gemma_lora/checkpoint-186/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-09-12 03:56:24,976 >> Special tokens file saved in gemma_lora/checkpoint-186/special_tokens_map.json\n",
            "[INFO|trainer.py:2394] 2024-09-12 03:56:25,515 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 504.6205, 'train_samples_per_second': 2.973, 'train_steps_per_second': 0.369, 'train_loss': 0.9869081025482506, 'epoch': 2.98}\n",
            "100% 186/186 [08:24<00:00,  2.71s/it]\n",
            "[INFO|trainer.py:3503] 2024-09-12 03:56:25,518 >> Saving model checkpoint to gemma_lora\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:25,767 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:25,768 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-09-12 03:56:25,870 >> tokenizer config file saved in gemma_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-09-12 03:56:25,870 >> Special tokens file saved in gemma_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.976\n",
            "  total_flos               =  4060908GF\n",
            "  train_loss               =     0.9869\n",
            "  train_runtime            = 0:08:24.62\n",
            "  train_samples_per_second =      2.973\n",
            "  train_steps_per_second   =      0.369\n",
            "[INFO|modelcard.py:449] 2024-09-12 03:56:26,257 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    stage=\"sft\",  # do supervised fine-tuning\n",
        "    do_train=True,\n",
        "    model_name_or_path=\"google/gemma-2b\",  # use bnb-4bit-quantized Gemma 2B model\n",
        "    dataset=\"alpaca_en_demo\",  # use the demo alpaca datasets\n",
        "    template=\"gemma\",  # use Gemma prompt template\n",
        "    finetuning_type=\"lora\",  # use LoRA adapters to save memory\n",
        "    lora_target=\"all\",  # attach LoRA adapters to all linear layers\n",
        "    output_dir=\"gemma_lora\",  # the path to save LoRA adapters\n",
        "    per_device_train_batch_size=2,  # the batch size\n",
        "    gradient_accumulation_steps=4,  # the gradient accumulation steps\n",
        "    lr_scheduler_type=\"cosine\",  # use cosine learning rate scheduler\n",
        "    logging_steps=10,  # log every 10 steps\n",
        "    warmup_ratio=0.1,  # use warmup scheduler\n",
        "    save_steps=1000,  # save checkpoint every 1000 steps\n",
        "    learning_rate=5e-5,  # the learning rate\n",
        "    num_train_epochs=3.0,  # the epochs of training\n",
        "    max_samples=500,  # use 500 examples in each dataset\n",
        "    max_grad_norm=1.0,  # clip gradient norm to 1.0\n",
        "    quantization_bit=4,  # use 4-bit QLoRA\n",
        "    loraplus_lr_ratio=16.0,  # use LoRA+ algorithm with lambda=16.0\n",
        "    fp16=True,  # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_gemma.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJbdNtZrANr"
      },
      "source": [
        "### Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2pGX3hLubhkJ",
        "outputId": "e32b9f86-c090-4dd7-fdc7-16b7f4ff84e8",
        "colab": {
          "referenced_widgets": [
            "597a71128f0245ee9bf09bbb2a4ce43d",
            "3018d8c138314e768c4b873b10564fea",
            "c7f2eb99c8494d52a7539526b873ce69",
            "2726e13ad97d41ac868cc3b0924a239b",
            "e66257d109464a72bb7bcc948747894b",
            "8551f98d1ea74017ba7d0d1fd8f80615",
            "aca1b1bb995b43bfb7240aba3f99ff02",
            "396a46f37c3549a5b7ce84c6263b0874",
            "9a37a1988382480ba7b91b4b29fcdd14",
            "c61ab84c59474a3fad6392e74ff95691",
            "34c040748e0e4f6da5bf32e8288162e0"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory/src\n",
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:36,380 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:36,383 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:36,544 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:36,545 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:36,547 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:36,548 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:36,548 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:37,772 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:37,774 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:37,880 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:37,881 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:37,882 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:37,883 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:37,884 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:38,715 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:38,717 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/12/2024 03:56:38 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "09/12/2024 03:56:38 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:3678] 2024-09-12 03:56:38,762 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1606] 2024-09-12 03:56:38,764 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:56:38,767 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-09-12 03:56:38,771 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "597a71128f0245ee9bf09bbb2a4ce43d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:4507] 2024-09-12 03:56:41,388 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-09-12 03:56:41,389 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-09-12 03:56:41,504 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:56:41,505 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09/12/2024 03:56:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "09/12/2024 03:56:41 - INFO - llamafactory.model.adapter - Loaded adapter(s): gemma_lora\n",
            "09/12/2024 03:56:41 - INFO - llamafactory.model.loader - all params: 2,515,978,240\n",
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory/src/\n",
        "\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b\",  # use Gemma 2B model\n",
        "    adapter_name_or_path=\"gemma_lora\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    quantization_bit=4,  # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\n",
        "    \"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\"\n",
        ")\n",
        "while True:\n",
        "    query = input(\"\\nUser: \")\n",
        "    if query.strip() == \"exit\":\n",
        "        break\n",
        "    if query.strip() == \"clear\":\n",
        "        messages = []\n",
        "        torch_gc()\n",
        "        print(\"History has been removed.\")\n",
        "        continue\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "    print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for new_text in chat_model.stream_chat(messages):\n",
        "        print(new_text, end=\"\", flush=True)\n",
        "        response += new_text\n",
        "    print()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZUSRbHhbV2"
      },
      "source": [
        "### Merge LoRA adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w84le7s5jyY_",
        "outputId": "761211b0-e151-45cb-8fd9-f79505a0d78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-09-12 03:56:50.782557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-12 03:56:50.801888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-12 03:56:50.807756: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-12 03:56:51.906658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:56,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:56,920 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:57,030 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:57,030 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:57,030 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:57,030 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:57,030 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:58,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:58,212 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:58,320 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:58,320 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:58,320 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:58,320 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-12 03:56:58,320 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-12 03:56:59,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-12 03:56:59,212 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "09/12/2024 03:56:59 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3678] 2024-09-12 03:56:59,246 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1606] 2024-09-12 03:56:59,247 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:56:59,248 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-09-12 03:56:59,250 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  3.47it/s]\n",
            "[INFO|modeling_utils.py:4507] 2024-09-12 03:56:59,865 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-09-12 03:56:59,865 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-09-12 03:57:00,012 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-09-12 03:57:00,013 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "09/12/2024 03:57:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "09/12/2024 03:57:09 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "09/12/2024 03:57:09 - INFO - llamafactory.model.adapter - Loaded adapter(s): gemma_lora\n",
            "09/12/2024 03:57:09 - INFO - llamafactory.model.loader - all params: 2,506,172,416\n",
            "09/12/2024 03:57:09 - INFO - llamafactory.train.tuner - Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:472] 2024-09-12 03:57:10,180 >> Configuration saved in gemma_lora_merged/config.json\n",
            "[INFO|configuration_utils.py:807] 2024-09-12 03:57:10,180 >> Configuration saved in gemma_lora_merged/generation_config.json\n",
            "[INFO|modeling_utils.py:2807] 2024-09-12 03:57:22,813 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at gemma_lora_merged/model.safetensors.index.json.\n",
            "README.md: 100% 5.19k/5.19k [00:00<00:00, 32.5MB/s]\n",
            "[INFO|configuration_utils.py:472] 2024-09-12 03:57:23,163 >> Configuration saved in /tmp/tmpw71gc8fw/config.json\n",
            "[INFO|configuration_utils.py:807] 2024-09-12 03:57:23,163 >> Configuration saved in /tmp/tmpw71gc8fw/generation_config.json\n",
            "[INFO|modeling_utils.py:2807] 2024-09-12 03:57:37,711 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpw71gc8fw/model.safetensors.index.json.\n",
            "[INFO|hub.py:798] 2024-09-12 03:57:50,991 >> Uploading the following files to schumbar/gemma-2b-finetuned-model-llama-factory: config.json,model-00002-of-00003.safetensors,model.safetensors.index.json,model-00001-of-00003.safetensors,model-00003-of-00003.safetensors,generation_config.json,README.md\n",
            "model-00002-of-00003.safetensors:   0% 0.00/1.98G [00:00<?, ?B/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/1.08G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Upload 3 LFS files:   0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 4.87M/1.08G [00:00<00:24, 43.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 4.78M/1.98G [00:00<02:10, 15.2MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 6.65M/1.95G [00:00<01:42, 18.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 7.73M/1.98G [00:00<01:53, 17.4MB/s]\n",
            "model-00002-of-00003.safetensors:   1% 10.4M/1.98G [00:00<01:38, 20.1MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 13.0M/1.08G [00:00<00:48, 22.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 12.5M/1.98G [00:00<02:09, 15.2MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 16.0M/1.08G [00:00<01:14, 14.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 16.0M/1.95G [00:00<02:22, 13.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 22.5M/1.08G [00:01<00:48, 21.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 16.0M/1.98G [00:01<03:22, 9.72MB/s]\n",
            "model-00001-of-00003.safetensors:   1% 26.4M/1.95G [00:01<01:29, 21.4MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 27.8M/1.08G [00:01<00:50, 21.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 29.0M/1.95G [00:01<01:31, 20.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 22.3M/1.98G [00:01<02:06, 15.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 34.0M/1.08G [00:01<01:11, 14.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 26.8M/1.98G [00:01<02:31, 12.9MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 28.5M/1.98G [00:02<02:27, 13.2MB/s]\n",
            "model-00002-of-00003.safetensors:   2% 31.9M/1.98G [00:02<02:07, 15.3MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 49.1M/1.08G [00:02<00:41, 25.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 43.6M/1.95G [00:02<01:37, 19.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 57.4M/1.08G [00:02<00:30, 33.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 45.8M/1.98G [00:02<01:15, 25.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 62.1M/1.08G [00:02<00:42, 24.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 50.0M/1.98G [00:02<01:22, 23.4MB/s]\n",
            "model-00002-of-00003.safetensors:   3% 56.1M/1.98G [00:03<01:09, 27.8MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 65.8M/1.08G [00:03<00:56, 18.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 59.0M/1.95G [00:03<01:42, 18.4MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 73.0M/1.08G [00:03<00:42, 23.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 63.3M/1.95G [00:03<01:27, 21.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 59.9M/1.98G [00:03<02:01, 15.8MB/s]\n",
            "model-00001-of-00003.safetensors:   3% 65.8M/1.95G [00:03<02:17, 13.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 78.0M/1.95G [00:03<01:05, 28.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 80.0M/1.08G [00:04<01:09, 14.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 64.0M/1.98G [00:04<02:33, 12.5MB/s]\n",
            "model-00001-of-00003.safetensors:   4% 83.0M/1.95G [00:04<01:19, 23.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 70.9M/1.98G [00:04<01:48, 17.6MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 91.0M/1.08G [00:04<00:54, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 95.3M/1.08G [00:04<00:47, 20.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 98.0M/1.95G [00:04<01:04, 28.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 77.8M/1.98G [00:04<02:12, 14.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 98.3M/1.08G [00:04<01:08, 14.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 110M/1.95G [00:05<01:11, 25.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 103M/1.08G [00:05<00:53, 18.2MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 80.1M/1.98G [00:05<03:02, 10.4MB/s]\n",
            "model-00001-of-00003.safetensors:   6% 113M/1.95G [00:05<01:47, 17.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 92.4M/1.98G [00:05<01:27, 21.5MB/s]\n",
            "model-00001-of-00003.safetensors:   6% 116M/1.95G [00:05<01:39, 18.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 110M/1.98G [00:05<00:53, 35.1MB/s] \n",
            "model-00001-of-00003.safetensors:   6% 125M/1.95G [00:05<01:28, 20.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 112M/1.08G [00:05<01:23, 11.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   6% 128M/1.98G [00:06<00:42, 43.5MB/s]\n",
            "model-00001-of-00003.safetensors:   7% 128M/1.95G [00:06<02:15, 13.4MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 128M/1.08G [00:06<00:50, 18.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 142M/1.95G [00:06<01:00, 29.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 135M/1.98G [00:06<00:54, 34.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 140M/1.98G [00:06<00:50, 36.6MB/s]\n",
            "model-00001-of-00003.safetensors:   8% 148M/1.95G [00:06<01:05, 27.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 145M/1.98G [00:07<01:03, 29.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 155M/1.98G [00:07<00:46, 39.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 151M/1.08G [00:07<00:37, 24.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 160M/1.95G [00:07<01:10, 25.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 161M/1.98G [00:07<00:56, 32.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 174M/1.98G [00:07<00:37, 48.0MB/s]\n",
            "model-00001-of-00003.safetensors:   9% 183M/1.95G [00:07<00:46, 37.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 181M/1.98G [00:07<00:55, 32.2MB/s]\n",
            "model-00001-of-00003.safetensors:  10% 189M/1.95G [00:08<01:01, 28.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 191M/1.98G [00:08<01:06, 27.1MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 171M/1.08G [00:08<00:49, 18.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 193M/1.95G [00:08<01:33, 18.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 195M/1.98G [00:08<01:30, 19.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 177M/1.08G [00:08<00:58, 15.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 208M/1.98G [00:09<00:54, 32.3MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 180M/1.08G [00:09<00:53, 16.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 213M/1.98G [00:09<01:02, 28.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 187M/1.08G [00:09<00:47, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 191M/1.08G [00:09<00:42, 20.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 209M/1.95G [00:09<01:56, 14.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 223M/1.95G [00:09<00:59, 28.9MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 193M/1.08G [00:09<01:05, 13.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 229M/1.95G [00:10<01:07, 25.4MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 203M/1.08G [00:10<00:36, 23.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 228M/1.98G [00:10<01:23, 21.0MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 208M/1.08G [00:10<00:40, 21.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 240M/1.95G [00:10<01:05, 26.2MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 219M/1.08G [00:10<00:25, 34.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 254M/1.98G [00:10<00:44, 38.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 225M/1.08G [00:10<00:29, 28.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 251M/1.95G [00:10<01:07, 25.3MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 237M/1.08G [00:10<00:19, 43.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 270M/1.98G [00:11<00:41, 41.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 244M/1.08G [00:11<00:23, 35.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  14% 276M/1.98G [00:11<00:46, 36.6MB/s]\n",
            "model-00002-of-00003.safetensors:  14% 287M/1.98G [00:11<00:35, 47.7MB/s]\n",
            "model-00001-of-00003.safetensors:  14% 265M/1.95G [00:11<01:09, 24.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 294M/1.98G [00:11<00:42, 39.3MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 300M/1.98G [00:11<00:39, 42.2MB/s]\n",
            "model-00001-of-00003.safetensors:  14% 272M/1.95G [00:11<01:10, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 278M/1.95G [00:11<00:59, 28.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 305M/1.98G [00:12<00:45, 36.6MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 311M/1.98G [00:12<00:41, 40.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 317M/1.98G [00:12<00:37, 43.9MB/s]\n",
            "model-00001-of-00003.safetensors:  15% 288M/1.95G [00:12<01:01, 27.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 302M/1.95G [00:12<00:38, 43.0MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 291M/1.08G [00:12<00:23, 34.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 301M/1.08G [00:12<00:17, 45.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 309M/1.95G [00:12<00:46, 35.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 306M/1.08G [00:12<00:22, 34.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 316M/1.08G [00:12<00:16, 45.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 320M/1.95G [00:13<00:46, 35.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 323M/1.98G [00:13<01:42, 16.2MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  17% 331M/1.98G [00:13<01:12, 22.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 329M/1.08G [00:13<00:19, 38.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 336M/1.98G [00:13<01:21, 20.2MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 351M/1.98G [00:13<00:46, 35.3MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 345M/1.08G [00:13<00:18, 39.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 358M/1.98G [00:13<00:50, 32.0MB/s]\n",
            "model-00002-of-00003.safetensors:  18% 364M/1.98G [00:14<00:45, 35.8MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 352M/1.08G [00:14<00:25, 28.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 380M/1.98G [00:14<00:37, 43.2MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 373M/1.08G [00:14<00:19, 37.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 386M/1.98G [00:14<00:44, 35.6MB/s]\n",
            "model-00002-of-00003.safetensors:  20% 392M/1.98G [00:14<00:40, 39.3MB/s]\n",
            "model-00002-of-00003.safetensors:  20% 398M/1.98G [00:14<00:38, 41.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 384M/1.08G [00:14<00:20, 34.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 403M/1.98G [00:15<00:47, 33.2MB/s]\n",
            "model-00001-of-00003.safetensors:  20% 387M/1.95G [00:15<00:55, 28.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 411M/1.98G [00:15<00:39, 40.2MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 401M/1.08G [00:15<00:18, 36.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 398M/1.95G [00:15<00:43, 36.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  21% 416M/1.98G [00:15<00:55, 28.2MB/s]\n",
            "model-00001-of-00003.safetensors:  21% 403M/1.95G [00:15<00:52, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 424M/1.98G [00:15<00:42, 36.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 416M/1.08G [00:15<00:19, 34.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 431M/1.98G [00:15<00:39, 39.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 422M/1.08G [00:15<00:18, 35.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 436M/1.98G [00:16<00:43, 35.5MB/s]\n",
            "model-00002-of-00003.safetensors:  23% 448M/1.98G [00:16<00:30, 49.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 437M/1.08G [00:16<00:17, 37.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 426M/1.95G [00:16<00:47, 32.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 454M/1.98G [00:16<00:42, 36.2MB/s]\n",
            "model-00001-of-00003.safetensors:  22% 432M/1.95G [00:16<01:02, 24.3MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 448M/1.08G [00:16<00:20, 30.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 443M/1.95G [00:16<00:39, 38.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 464M/1.98G [00:16<00:45, 33.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 470M/1.98G [00:16<00:41, 36.2MB/s]\n",
            "model-00001-of-00003.safetensors:  23% 449M/1.95G [00:17<00:47, 31.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 460M/1.95G [00:17<00:34, 43.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 468M/1.08G [00:17<00:19, 32.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  25% 489M/1.98G [00:17<00:35, 41.9MB/s]\n",
            "model-00001-of-00003.safetensors:  24% 466M/1.95G [00:17<00:41, 36.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 473M/1.95G [00:17<00:36, 40.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 483M/1.08G [00:17<00:19, 31.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  46% 496M/1.08G [00:17<00:12, 46.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 480M/1.95G [00:17<00:42, 34.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 496M/1.98G [00:17<00:56, 26.5MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 508M/1.98G [00:18<00:40, 36.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 508M/1.08G [00:18<00:14, 38.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 501M/1.95G [00:18<00:35, 40.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 527M/1.98G [00:18<00:32, 45.1MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 513M/1.08G [00:18<00:20, 27.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  49% 527M/1.08G [00:18<00:12, 45.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 516M/1.95G [00:18<00:40, 35.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 527M/1.95G [00:18<00:30, 46.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 534M/1.98G [00:19<00:54, 26.4MB/s]\n",
            "model-00002-of-00003.safetensors:  27% 542M/1.98G [00:19<00:45, 31.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 544M/1.08G [00:19<00:15, 35.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 539M/1.95G [00:19<00:42, 33.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 551M/1.08G [00:19<00:13, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 548M/1.98G [00:19<00:50, 28.3MB/s]\n",
            "model-00002-of-00003.safetensors:  28% 560M/1.98G [00:19<00:35, 39.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 563M/1.08G [00:19<00:14, 36.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 555M/1.95G [00:19<00:36, 38.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 566M/1.98G [00:19<00:41, 33.8MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 574M/1.98G [00:19<00:35, 39.7MB/s]\n",
            "model-00001-of-00003.safetensors:  29% 560M/1.95G [00:20<00:46, 29.8MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  54% 580M/1.08G [00:20<00:14, 35.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 574M/1.95G [00:20<00:30, 45.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 585M/1.98G [00:20<00:38, 36.3MB/s]\n",
            "model-00001-of-00003.safetensors:  30% 581M/1.95G [00:20<00:39, 34.9MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 592M/1.08G [00:20<00:15, 31.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 589M/1.95G [00:20<00:32, 41.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 592M/1.98G [00:20<00:54, 25.7MB/s]\n",
            "model-00002-of-00003.safetensors:  30% 603M/1.98G [00:20<00:37, 36.7MB/s]\n",
            "model-00001-of-00003.safetensors:  31% 601M/1.95G [00:20<00:35, 37.9MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 608M/1.08G [00:20<00:14, 31.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 606M/1.95G [00:21<00:33, 39.9MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 614M/1.08G [00:21<00:13, 34.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 622M/1.98G [00:21<00:30, 45.3MB/s]\n",
            "model-00001-of-00003.safetensors:  31% 611M/1.95G [00:21<00:42, 31.2MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 629M/1.08G [00:21<00:11, 37.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 618M/1.95G [00:21<00:35, 37.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 629M/1.98G [00:21<00:38, 35.3MB/s]\n",
            "model-00002-of-00003.safetensors:  32% 640M/1.98G [00:21<00:28, 46.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 640M/1.08G [00:21<00:13, 31.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 633M/1.95G [00:21<00:32, 40.8MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 649M/1.08G [00:21<00:10, 41.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 647M/1.98G [00:22<00:33, 39.9MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 653M/1.98G [00:22<00:31, 41.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 659M/1.08G [00:22<00:12, 33.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 644M/1.95G [00:22<00:44, 29.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 665M/1.08G [00:22<00:10, 38.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 658M/1.98G [00:22<00:44, 29.9MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 663M/1.98G [00:22<00:40, 32.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 677M/1.08G [00:22<00:12, 33.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 672M/1.98G [00:23<00:49, 26.7MB/s]\n",
            "model-00001-of-00003.safetensors:  34% 671M/1.95G [00:23<00:34, 37.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 688M/1.98G [00:23<00:32, 40.4MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 697M/1.08G [00:23<00:09, 39.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 677M/1.95G [00:23<00:39, 32.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 693M/1.98G [00:23<00:38, 33.4MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 702M/1.98G [00:23<00:29, 43.6MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 711M/1.08G [00:23<00:09, 38.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 692M/1.95G [00:23<00:37, 33.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 709M/1.98G [00:23<00:41, 30.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 720M/1.08G [00:23<00:11, 32.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 704M/1.95G [00:24<00:38, 32.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 727M/1.08G [00:24<00:09, 38.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 711M/1.95G [00:24<00:32, 38.4MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 733M/1.08G [00:24<00:08, 40.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 734M/1.98G [00:24<00:27, 44.6MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 738M/1.08G [00:24<00:10, 31.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 751M/1.08G [00:24<00:06, 48.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 723M/1.95G [00:24<00:40, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 741M/1.98G [00:24<00:34, 36.0MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 757M/1.08G [00:24<00:08, 36.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 752M/1.98G [00:25<00:39, 31.4MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 767M/1.98G [00:25<00:26, 45.4MB/s]\n",
            "model-00001-of-00003.safetensors:  39% 752M/1.95G [00:25<00:36, 32.8MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 777M/1.08G [00:25<00:07, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 784M/1.08G [00:25<00:06, 45.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 781M/1.98G [00:25<00:28, 42.4MB/s]\n",
            "model-00001-of-00003.safetensors:  39% 768M/1.95G [00:25<00:36, 32.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  40% 787M/1.98G [00:25<00:31, 38.1MB/s]\n",
            "model-00001-of-00003.safetensors:  40% 777M/1.95G [00:25<00:29, 39.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  40% 792M/1.98G [00:26<00:31, 37.3MB/s]\n",
            "model-00002-of-00003.safetensors:  40% 797M/1.98G [00:26<00:30, 39.0MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 801M/1.08G [00:26<00:10, 25.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 801M/1.98G [00:26<00:38, 30.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 810M/1.08G [00:26<00:07, 35.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 807M/1.98G [00:26<00:35, 32.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 816M/1.08G [00:26<00:07, 35.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 813M/1.98G [00:26<00:31, 36.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 821M/1.08G [00:26<00:09, 29.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 801M/1.95G [00:26<00:42, 26.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 817M/1.98G [00:26<00:45, 25.5MB/s]\n",
            "model-00002-of-00003.safetensors:  41% 822M/1.98G [00:27<00:41, 28.2MB/s]\n",
            "model-00002-of-00003.safetensors:  42% 830M/1.98G [00:27<00:30, 38.0MB/s]\n",
            "model-00002-of-00003.safetensors:  42% 835M/1.98G [00:27<00:34, 33.3MB/s]\n",
            "model-00002-of-00003.safetensors:  42% 841M/1.98G [00:27<00:29, 38.1MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 834M/1.08G [00:27<00:11, 21.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 841M/1.08G [00:27<00:08, 26.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 836M/1.95G [00:27<00:30, 36.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 860M/1.98G [00:27<00:24, 45.6MB/s]\n",
            "model-00001-of-00003.safetensors:  44% 848M/1.95G [00:28<00:32, 33.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 848M/1.08G [00:28<00:10, 22.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 857M/1.95G [00:28<00:25, 43.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  44% 866M/1.98G [00:28<00:30, 36.3MB/s]\n",
            "model-00002-of-00003.safetensors:  44% 871M/1.98G [00:28<00:30, 36.6MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 864M/1.08G [00:28<00:07, 27.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 869M/1.95G [00:28<00:27, 39.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 872M/1.08G [00:28<00:06, 34.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 880M/1.98G [00:28<00:32, 33.6MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 887M/1.98G [00:28<00:28, 38.7MB/s]\n",
            "model-00001-of-00003.safetensors:  45% 881M/1.95G [00:28<00:35, 30.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 883M/1.08G [00:28<00:06, 29.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 896M/1.98G [00:29<00:30, 35.6MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 903M/1.98G [00:29<00:27, 39.8MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 909M/1.98G [00:29<00:24, 43.7MB/s]\n",
            "model-00001-of-00003.safetensors:  46% 898M/1.95G [00:29<00:30, 34.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 908M/1.95G [00:29<00:23, 45.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 914M/1.98G [00:29<00:31, 33.8MB/s]\n",
            "model-00001-of-00003.safetensors:  47% 914M/1.95G [00:29<00:26, 38.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 921M/1.98G [00:29<00:28, 37.7MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 926M/1.98G [00:29<00:26, 39.1MB/s]\n",
            "model-00001-of-00003.safetensors:  47% 919M/1.95G [00:29<00:27, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 925M/1.95G [00:29<00:24, 41.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 937M/1.98G [00:30<00:27, 37.6MB/s]\n",
            "model-00001-of-00003.safetensors:  48% 930M/1.95G [00:30<00:30, 33.3MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 923M/1.08G [00:30<00:04, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 942M/1.98G [00:30<00:27, 37.9MB/s]\n",
            "model-00001-of-00003.safetensors:  48% 935M/1.95G [00:30<00:30, 33.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 943M/1.95G [00:30<00:23, 42.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 946M/1.98G [00:30<00:34, 30.1MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 952M/1.98G [00:30<00:28, 35.8MB/s]\n",
            "model-00001-of-00003.safetensors:  49% 948M/1.95G [00:30<00:30, 32.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 957M/1.98G [00:30<00:27, 37.2MB/s]\n",
            "model-00001-of-00003.safetensors:  49% 954M/1.95G [00:30<00:28, 34.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 961M/1.98G [00:31<00:37, 26.9MB/s]\n",
            "model-00001-of-00003.safetensors:  49% 960M/1.95G [00:31<00:31, 31.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 967M/1.98G [00:31<00:32, 31.0MB/s]\n",
            "model-00002-of-00003.safetensors:  49% 974M/1.98G [00:31<00:28, 35.3MB/s]\n",
            "model-00001-of-00003.safetensors:  50% 973M/1.95G [00:31<00:24, 39.2MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  89% 962M/1.08G [00:31<00:04, 29.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 975M/1.08G [00:31<00:02, 48.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 978M/1.98G [00:31<00:39, 25.7MB/s]\n",
            "model-00002-of-00003.safetensors:  50% 985M/1.98G [00:31<00:30, 32.4MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 990M/1.98G [00:31<00:27, 35.5MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  91% 988M/1.08G [00:31<00:02, 40.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 994M/1.98G [00:32<00:37, 26.2MB/s]\n",
            "model-00001-of-00003.safetensors:  52% 1.01G/1.95G [00:32<00:20, 45.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.01G/1.98G [00:32<00:24, 40.0MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.01G/1.98G [00:32<00:30, 32.3MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 1.01G/1.08G [00:32<00:02, 32.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  94% 1.02G/1.08G [00:32<00:01, 41.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 1.02G/1.98G [00:32<00:27, 34.9MB/s]\n",
            "model-00002-of-00003.safetensors:  52% 1.03G/1.98G [00:32<00:22, 42.6MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  52% 1.04G/1.98G [00:33<00:21, 43.9MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 1.03G/1.08G [00:33<00:01, 43.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.03G/1.95G [00:33<00:35, 25.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 1.05G/1.98G [00:33<00:20, 44.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 1.04G/1.08G [00:33<00:01, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 1.05G/1.08G [00:33<00:00, 42.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 1.07G/1.98G [00:33<00:18, 50.1MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 1.06G/1.08G [00:33<00:00, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  99% 1.07G/1.08G [00:33<00:00, 47.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 1.08G/1.98G [00:34<00:23, 38.2MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors:  99% 1.07G/1.08G [00:34<00:00, 35.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 1.08G/1.08G [00:34<00:00, 31.3MB/s]\n",
            "model-00002-of-00003.safetensors:  56% 1.11G/1.98G [00:34<00:23, 37.7MB/s]\n",
            "model-00002-of-00003.safetensors:  57% 1.12G/1.98G [00:35<00:26, 32.9MB/s]\n",
            "model-00002-of-00003.safetensors:  57% 1.13G/1.98G [00:35<00:22, 38.2MB/s]\n",
            "model-00001-of-00003.safetensors:  55% 1.07G/1.95G [00:35<00:49, 17.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 1.15G/1.98G [00:36<00:21, 39.1MB/s]\n",
            "model-00002-of-00003.safetensors:  58% 1.16G/1.98G [00:36<00:23, 35.5MB/s]\n",
            "model-00001-of-00003.safetensors:  57% 1.10G/1.95G [00:36<00:31, 26.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 1.18G/1.98G [00:36<00:18, 42.8MB/s]\n",
            "model-00002-of-00003.safetensors:  60% 1.19G/1.98G [00:37<00:20, 39.6MB/s]\n",
            "model-00002-of-00003.safetensors:  61% 1.20G/1.98G [00:37<00:22, 35.3MB/s]\n",
            "model-00002-of-00003.safetensors:  61% 1.21G/1.98G [00:37<00:17, 44.0MB/s]\n",
            "model-00002-of-00003.safetensors:  62% 1.23G/1.98G [00:38<00:14, 51.1MB/s]\n",
            "model-00002-of-00003.safetensors:  63% 1.25G/1.98G [00:38<00:18, 40.1MB/s]\n",
            "model-00001-of-00003.safetensors:  60% 1.17G/1.95G [00:38<00:39, 19.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 1.28G/1.98G [00:39<00:13, 54.0MB/s]\n",
            "model-00002-of-00003.safetensors:  65% 1.29G/1.98G [00:39<00:16, 41.9MB/s]\n",
            "model-00001-of-00003.safetensors:  62% 1.20G/1.95G [00:39<00:25, 29.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 1.31G/1.98G [00:39<00:13, 48.1MB/s]\n",
            "model-00002-of-00003.safetensors:  67% 1.32G/1.98G [00:40<00:15, 42.8MB/s]\n",
            "model-00002-of-00003.safetensors:  67% 1.33G/1.98G [00:40<00:18, 36.0MB/s]\n",
            "model-00002-of-00003.safetensors:  67% 1.34G/1.98G [00:40<00:15, 40.6MB/s]\n",
            "model-00001-of-00003.safetensors:  64% 1.25G/1.95G [00:40<00:18, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 1.26G/1.95G [00:41<00:20, 33.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 1.28G/1.95G [00:41<00:14, 47.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 1.29G/1.95G [00:41<00:14, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.30G/1.95G [00:41<00:18, 35.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.31G/1.95G [00:42<00:12, 49.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 1.32G/1.95G [00:42<00:15, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 1.33G/1.95G [00:42<00:18, 34.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 1.34G/1.95G [00:42<00:12, 48.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 1.35G/1.95G [00:43<00:14, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 1.36G/1.95G [00:43<00:15, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.37G/1.95G [00:43<00:10, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.38G/1.95G [00:43<00:12, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.39G/1.95G [00:44<00:13, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 1.41G/1.95G [00:44<00:09, 56.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 1.41G/1.98G [00:46<00:19, 30.1MB/s]\n",
            "model-00001-of-00003.safetensors:  73% 1.42G/1.95G [00:47<00:55, 9.48MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 1.41G/1.98G [00:47<00:28, 19.6MB/s]\n",
            "model-00001-of-00003.safetensors:  74% 1.44G/1.95G [00:48<00:34, 14.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 1.44G/1.98G [00:48<00:17, 31.0MB/s]\n",
            "model-00002-of-00003.safetensors:  73% 1.45G/1.98G [00:48<00:18, 29.2MB/s]\n",
            "model-00001-of-00003.safetensors:  76% 1.47G/1.95G [00:48<00:20, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 1.47G/1.98G [00:49<00:13, 37.6MB/s]\n",
            "model-00001-of-00003.safetensors:  77% 1.49G/1.95G [00:49<00:15, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 1.50G/1.95G [00:49<00:14, 31.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 1.52G/1.95G [00:49<00:09, 44.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 1.53G/1.95G [00:49<00:10, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 1.54G/1.95G [00:50<00:10, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 1.55G/1.95G [00:50<00:07, 52.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 1.48G/1.98G [00:50<00:34, 14.7MB/s]\n",
            "model-00001-of-00003.safetensors:  80% 1.57G/1.95G [00:50<00:10, 36.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 1.50G/1.98G [00:51<00:18, 26.2MB/s]\n",
            "model-00002-of-00003.safetensors:  76% 1.51G/1.98G [00:51<00:18, 25.1MB/s]\n",
            "model-00001-of-00003.safetensors:  82% 1.60G/1.95G [00:51<00:09, 35.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 1.53G/1.98G [00:52<00:12, 36.7MB/s]\n",
            "model-00002-of-00003.safetensors:  78% 1.54G/1.98G [00:52<00:12, 34.0MB/s]\n",
            "model-00001-of-00003.safetensors:  84% 1.63G/1.95G [00:52<00:08, 35.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 1.57G/1.98G [00:52<00:09, 43.0MB/s]\n",
            "model-00002-of-00003.safetensors:  79% 1.57G/1.98G [00:53<00:11, 36.9MB/s]\n",
            "model-00002-of-00003.safetensors:  80% 1.58G/1.98G [00:53<00:09, 42.1MB/s]\n",
            "model-00002-of-00003.safetensors:  81% 1.60G/1.98G [00:53<00:11, 34.2MB/s]\n",
            "model-00002-of-00003.safetensors:  81% 1.61G/1.98G [00:54<00:08, 44.8MB/s]\n",
            "model-00001-of-00003.safetensors:  87% 1.69G/1.95G [00:54<00:08, 31.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 1.63G/1.98G [00:54<00:07, 49.2MB/s]\n",
            "model-00002-of-00003.safetensors:  83% 1.64G/1.98G [00:54<00:09, 37.7MB/s]\n",
            "model-00001-of-00003.safetensors:  88% 1.71G/1.95G [00:54<00:07, 32.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 1.65G/1.98G [00:54<00:08, 41.6MB/s]\n",
            "model-00002-of-00003.safetensors:  84% 1.67G/1.98G [00:55<00:08, 35.2MB/s]\n",
            "model-00002-of-00003.safetensors:  85% 1.68G/1.98G [00:55<00:07, 40.4MB/s]\n",
            "model-00002-of-00003.safetensors:  85% 1.68G/1.98G [00:56<00:08, 33.8MB/s]\n",
            "model-00002-of-00003.safetensors:  85% 1.69G/1.98G [00:56<00:06, 42.5MB/s]\n",
            "model-00002-of-00003.safetensors:  86% 1.71G/1.98G [00:56<00:05, 52.4MB/s]\n",
            "model-00001-of-00003.safetensors:  90% 1.76G/1.95G [00:56<00:07, 26.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 1.72G/1.98G [00:56<00:06, 41.3MB/s]\n",
            "model-00002-of-00003.safetensors:  88% 1.74G/1.98G [00:57<00:05, 47.5MB/s]\n",
            "model-00001-of-00003.safetensors:  92% 1.79G/1.95G [00:57<00:05, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 1.76G/1.98G [00:57<00:04, 47.0MB/s]\n",
            "model-00002-of-00003.safetensors:  89% 1.77G/1.98G [00:58<00:06, 33.9MB/s]\n",
            "model-00002-of-00003.safetensors:  90% 1.77G/1.98G [00:58<00:05, 40.4MB/s]\n",
            "model-00002-of-00003.safetensors:  90% 1.78G/1.98G [00:58<00:05, 35.8MB/s]\n",
            "model-00001-of-00003.safetensors:  94% 1.84G/1.95G [00:58<00:02, 37.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 1.82G/1.98G [00:59<00:03, 51.5MB/s]\n",
            "model-00002-of-00003.safetensors:  92% 1.83G/1.98G [00:59<00:03, 44.5MB/s]\n",
            "model-00001-of-00003.safetensors:  95% 1.86G/1.95G [00:59<00:04, 18.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 1.84G/1.98G [00:59<00:04, 30.3MB/s]\n",
            "model-00002-of-00003.safetensors:  93% 1.85G/1.98G [01:00<00:03, 40.3MB/s]\n",
            "model-00002-of-00003.safetensors:  94% 1.86G/1.98G [01:00<00:03, 31.4MB/s]\n",
            "model-00002-of-00003.safetensors:  94% 1.87G/1.98G [01:00<00:02, 41.3MB/s]\n",
            "model-00001-of-00003.safetensors:  97% 1.90G/1.95G [01:00<00:01, 32.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 1.90G/1.95G [01:00<00:01, 38.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 1.91G/1.95G [01:00<00:01, 32.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 1.88G/1.98G [01:01<00:04, 21.7MB/s]\n",
            "model-00002-of-00003.safetensors:  96% 1.90G/1.98G [01:01<00:02, 38.3MB/s]\n",
            "model-00001-of-00003.safetensors: 100% 1.94G/1.95G [01:01<00:00, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 1.95G/1.95G [01:02<00:00, 31.3MB/s]\n",
            "model-00002-of-00003.safetensors: 100% 1.98G/1.98G [01:04<00:00, 30.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 3 LFS files: 100% 3/3 [01:04<00:00, 21.60s/it]\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-09-12 03:58:58,242 >> tokenizer config file saved in gemma_lora_merged/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-09-12 03:58:58,242 >> Special tokens file saved in gemma_lora_merged/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-09-12 03:58:58,797 >> tokenizer config file saved in /tmp/tmpjf02z06e/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-09-12 03:58:58,797 >> Special tokens file saved in /tmp/tmpjf02z06e/special_tokens_map.json\n",
            "[INFO|hub.py:798] 2024-09-12 03:58:59,196 >> Uploading the following files to schumbar/gemma-2b-finetuned-model-llama-factory: tokenizer.json,README.md,special_tokens_map.json,tokenizer_config.json,tokenizer.model\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b\",  # use official non-quantized Gemma 2B model\n",
        "    adapter_name_or_path=\"gemma_lora\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    export_dir=\"gemma_lora_merged\",  # path to save the merged model\n",
        "    export_size=2,  # the file shard size (in GB) of the merged model\n",
        "    export_device=\"cpu\",  # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "    export_hub_model_id=\"gemma-2b-finetuned-model-llama-factory\",  # your Hugging Face hub model ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_gemma.json"
      ]
    }
  ]
}